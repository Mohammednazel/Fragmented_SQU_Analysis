{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda937ef-5f1e-4c07-a12d-c6b98f80c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set some display options for pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ebfa9c-49e7-4b72-9862-711ee26912d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data from the CSV file\n",
    "file_path = '../data/processed/cleaned_purchase_orders.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert created_date back to datetime object for time-series analysis\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fa5538-c193-4ce2-a81f-95dce6e834ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Section A: Performance Overview KPIs\n",
    "# -----------------------------\n",
    "required_cols = [\n",
    "'purchasing_group', 'plant', 'material_group', 'description', 'supplier_id',\n",
    "'product_id', 'quantity', 'unit_price', 'net_value', 'month', 'unit' , 'purchase_order_id' , 'created_date' , 'status'\n",
    "]\n",
    "\n",
    "\n",
    "for col in required_cols:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Warning: Column '{col}' not found in dataset\")\n",
    "\n",
    "\n",
    "# KPIs\n",
    "kpi_total_spend = df['net_value'].sum()\n",
    "kpi_total_quantity = df['quantity'].sum()\n",
    "total_skus = df['product_id'].nunique()\n",
    "\n",
    "\n",
    "# Fragmentation analysis\n",
    "supplier_counts_per_sku = df.groupby('product_id')['supplier_id'].nunique()\n",
    "kpi_fragmented_skus = (supplier_counts_per_sku > 1).sum()\n",
    "kpi_fragmentation_rate = kpi_fragmented_skus / total_skus if total_skus > 0 else 0\n",
    "\n",
    "# Saving Potential Calculation\n",
    "min_prices = df.groupby('product_id')['unit_price'].min().reset_index()\n",
    "min_prices.rename(columns={'unit_price': 'min_price'}, inplace=True)\n",
    "df_merged = pd.merge(df, min_prices, on='product_id')\n",
    "df_merged['potential_saving'] = (df_merged['unit_price'] - df_merged['min_price']) * df_merged['quantity']\n",
    "kpi_saving_potential = df_merged['potential_saving'].sum()\n",
    "\n",
    "# --- Additional KPIs ---\n",
    "\n",
    "# 1. Supplier-Related KPIs\n",
    "kpi_active_suppliers = df['supplier_id'].nunique()\n",
    "top_5_spend = df.groupby('supplier_id')['net_value'].sum().nlargest(5).sum()\n",
    "kpi_top_5_spend_pct = (top_5_spend / kpi_total_spend) * 100 if kpi_total_spend > 0 else 0\n",
    "kpi_single_sourced_skus = (supplier_counts_per_sku == 1).sum()\n",
    "\n",
    "# 2. Operational & Process KPIs\n",
    "kpi_total_pos = df['purchase_order_id'].nunique()\n",
    "po_values = df.groupby('purchase_order_id')['net_value'].sum()\n",
    "kpi_avg_po_value = po_values.mean()\n",
    "line_items_per_po = df.groupby('purchase_order_id').size()\n",
    "kpi_avg_line_items = line_items_per_po.mean()\n",
    "pending_pos = df[df['status'] == 'Pending']['purchase_order_id'].nunique()\n",
    "kpi_pending_rate_pct = (pending_pos / kpi_total_pos) * 100 if kpi_total_pos > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae92f53-5067-4156-b11b-4346f8d33c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly sorted Spend Trend:\n",
      "   created_date  net_value  quantity    month\n",
      "0       2025-01  107527.67  75573.00  2025-01\n",
      "1       2025-02  110545.33  80023.00  2025-02\n",
      "2       2025-03  312557.03 142140.00  2025-03\n",
      "3       2025-04   34987.49  15358.00  2025-04\n",
      "4       2025-05  231804.23 135536.00  2025-05\n",
      "5       2025-06  143897.72  94935.00  2025-06\n",
      "6       2025-07  248165.67 137692.00  2025-07\n",
      "7       2025-08  168856.99 115094.00  2025-08\n",
      "8       2025-09   63305.44  33519.00  2025-09\n",
      "9       2025-10  119131.15  69936.00  2025-10\n",
      "10      2025-11  117691.03  81776.00  2025-11\n",
      "11      2025-12   77346.41  49062.00  2025-12\n",
      "\n",
      "Supplier Scorecard (Who is cheapest for our Top SKUs):\n",
      "   product_id   supplier_id  best_price\n",
      "0    MAT-3159  SUPP_PACK_08        0.60\n",
      "1    MAT-4077  SUPP_PACK_06        3.01\n",
      "2    MAT-4862  SUPP_PACK_03        1.80\n",
      "4    MAT-5204  SUPP_PACK_07        1.21\n",
      "5    MAT-5204  SUPP_PACK_02        1.21\n",
      "7    MAT-5627  SUPP_PACK_02        0.83\n",
      "8    MAT-5627  SUPP_PACK_03        0.83\n",
      "9    MAT-6002  SUPP_PACK_02        2.00\n",
      "10   MAT-6214  SUPP_PACK_03        0.50\n",
      "11   MAT-6214  SUPP_PACK_05        0.50\n",
      "12   MAT-7729  SUPP_PACK_01        1.01\n",
      "Empty DataFrame\n",
      "Columns: [plant, net_value, quantity]\n",
      "Index: []\n",
      "  material_group  net_value  quantity\n",
      "4           MEAL  626921.18 281605.00\n",
      "3          GRAIN  410162.86 305619.00\n",
      "0            ADD  325611.72 152555.00\n",
      "5        OILSEED  226640.90 112500.00\n",
      "1         BYPROD   78707.96 108058.00\n",
      "2         FORAGE   67771.54  70307.00\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Section B: Analysis & Trends\n",
    "# -----------------------------\n",
    "# Spend trend by month\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "\n",
    "# Group by month using the actual date, then format the label\n",
    "spend_trend = df.groupby(df['created_date'].dt.to_period('M')).agg({\n",
    "    'net_value':'sum',\n",
    "    'quantity':'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort chronologically\n",
    "spend_trend = spend_trend.sort_values('created_date')\n",
    "\n",
    "# Convert the period to a more readable string format for the final report\n",
    "spend_trend['month'] = spend_trend['created_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "print(\"Correctly sorted Spend Trend:\")\n",
    "print(spend_trend)\n",
    "\n",
    "\n",
    "\n",
    "# Spend by purchasing_group\n",
    "purchasing_group_spend = df.groupby('purchasing_group').agg({\n",
    "    'net_value':'sum',\n",
    "    'quantity':'sum'\n",
    "}).reset_index().sort_values('net_value', ascending=False)\n",
    "\n",
    "\n",
    "# Top SKUs by spend\n",
    "top_skus = df.groupby(['product_id','description']).agg({\n",
    "    'net_value':'sum',\n",
    "    'quantity':'sum'\n",
    "}).reset_index().sort_values('net_value', ascending=False).head(10)\n",
    "\n",
    "\n",
    "# Supplier price analysis (avg unit prices)\n",
    "# A more meaningful supplier analysis\n",
    "\n",
    "# First, get the list of your top 10 SKUs by spend (from your previous step)\n",
    "top_10_sku_list = top_skus['product_id'].tolist()\n",
    "\n",
    "# Filter the main dataframe to only include these top SKUs\n",
    "top_skus_df = df[df['product_id'].isin(top_10_sku_list)]\n",
    "\n",
    "# Now, find the best price offered for each of these SKUs\n",
    "best_prices_per_sku = top_skus_df.groupby('product_id')['unit_price'].min().reset_index()\n",
    "best_prices_per_sku.rename(columns={'unit_price': 'best_price'}, inplace=True)\n",
    "\n",
    "# Merge this back to find which supplier offered that best price\n",
    "supplier_scorecard = pd.merge(\n",
    "    best_prices_per_sku,\n",
    "    top_skus_df[['product_id', 'supplier_id', 'unit_price']],\n",
    "    left_on=['product_id', 'best_price'],\n",
    "    right_on=['product_id', 'unit_price']\n",
    ")\n",
    "\n",
    "# Clean up and show the final scorecard\n",
    "supplier_scorecard = supplier_scorecard[['product_id', 'supplier_id', 'best_price']].drop_duplicates()\n",
    "\n",
    "print(\"\\nSupplier Scorecard (Who is cheapest for our Top SKUs):\")\n",
    "print(supplier_scorecard)\n",
    "\n",
    "\n",
    "# Spend by Plant\n",
    "plant_spend = df.groupby('plant').agg({\n",
    "    'net_value':'sum',\n",
    "    'quantity':'sum'\n",
    "}).reset_index().sort_values('net_value', ascending=False)\n",
    "\n",
    "print(plant_spend)\n",
    "\n",
    "# Spend by Material Group\n",
    "material_group_spend = df.groupby('material_group').agg({\n",
    "    'net_value':'sum',\n",
    "    'quantity':'sum'\n",
    "}).reset_index().sort_values('net_value', ascending=False)\n",
    "\n",
    "print(material_group_spend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b7d244-cd87-47fe-ae37-15ab6728721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Recommendations:\n",
      "  product_id          description Recommended Supplier  Best Price  \\\n",
      "0   MAT-6002         Soybean Meal         SUPP_PACK_02        2.00   \n",
      "2   MAT-4077  Rumen Protected Fat         SUPP_PACK_06        3.01   \n",
      "1   MAT-5204                 Corn         SUPP_PACK_07        1.21   \n",
      "\n",
      "   avg_unit_price  total_quantity  Estimated Saving  \\\n",
      "0            2.23       281605.00          63727.21   \n",
      "2            3.80        68252.00          54028.28   \n",
      "1            1.38       222894.00          38777.84   \n",
      "\n",
      "   purchase_frequency_months  supplier_count  \n",
      "0                         10               9  \n",
      "2                          8               7  \n",
      "1                         10               8  \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Section C: Procurement Recommendations (Revised)\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Create a summary table for each SKU\n",
    "sku_summary = df.groupby(['product_id', 'description']).agg(\n",
    "    total_quantity=('quantity', 'sum'),\n",
    "    total_net_value=('net_value', 'sum'),\n",
    "    avg_unit_price=('unit_price', 'mean'),\n",
    "    purchase_frequency_months=('month', 'nunique'),\n",
    "    supplier_count=('supplier_id', 'nunique')\n",
    ").reset_index().sort_values('total_net_value', ascending=False)\n",
    "\n",
    "# Add a cumulative spend column to identify top items\n",
    "sku_summary['cum_spend_pct'] = (sku_summary['total_net_value'].cumsum() / sku_summary['total_net_value'].sum()) * 100\n",
    "\n",
    "# 2. Identify high-value, recurring, fragmented SKUs as Contract Candidates\n",
    "contract_candidates = sku_summary[\n",
    "    (sku_summary['purchase_frequency_months'] >= 3) &\n",
    "    (sku_summary['supplier_count'] > 1) &\n",
    "    (sku_summary['cum_spend_pct'] <= 80) # <-- Focus on top 80% spend\n",
    "].copy() # Using .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# 3. Find the best (minimum) price for every SKU from the original data\n",
    "best_suppliers = df.loc[df.groupby('product_id')['unit_price'].idxmin()]\n",
    "best_suppliers = best_suppliers[['product_id', 'supplier_id', 'unit_price']]\n",
    "best_suppliers.rename(columns={'supplier_id': 'Recommended Supplier', 'unit_price': 'Best Price'}, inplace=True)\n",
    "\n",
    "# 4. Generate recommendations by merging candidates with best suppliers (NO LOOP)\n",
    "recommendations_df = pd.merge(contract_candidates, best_suppliers, on='product_id')\n",
    "\n",
    "# 5. Calculate estimated savings\n",
    "recommendations_df['Estimated Saving'] = (recommendations_df['avg_unit_price'] - recommendations_df['Best Price']) * recommendations_df['total_quantity']\n",
    "\n",
    "# 6. Clean up the final recommendations table\n",
    "recommendations_df = recommendations_df[[\n",
    "    'product_id', 'description', 'Recommended Supplier', 'Best Price',\n",
    "    'avg_unit_price', 'total_quantity', 'Estimated Saving', 'purchase_frequency_months', 'supplier_count'\n",
    "]].sort_values('Estimated Saving', ascending=False)\n",
    "\n",
    "print(\"\\nTop Recommendations:\")\n",
    "print(recommendations_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe264d09-dca1-4fa8-b067-605870566a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced SKU Analysis Table (showing highest overspending first):\n",
      "    purchasing_group product_id          description   supplier_id  \\\n",
      "100          PG-MEAL   MAT-6002         Soybean Meal  SUPP_PACK_03   \n",
      "98           PG-MEAL   MAT-6002         Soybean Meal  SUPP_PACK_01   \n",
      "26           PG-MEAL   MAT-4077  Rumen Protected Fat  SUPP_PACK_01   \n",
      "101          PG-MEAL   MAT-6002         Soybean Meal  SUPP_PACK_08   \n",
      "57          PG-GRAIN   MAT-5204                 Corn  SUPP_PACK_01   \n",
      "\n",
      "     quantity_purchased  contribution_to_total_qty_%  avg_price_paid  \\\n",
      "100            71020.00                        25.22            2.22   \n",
      "98             52822.00                        18.76            2.23   \n",
      "26             14865.00                        21.78            3.74   \n",
      "101            45671.00                        16.22            2.23   \n",
      "57             48913.00                        21.94            1.42   \n",
      "\n",
      "     best_available_price  price_variance  cost_above_best_price  \n",
      "100                  2.00            0.22               15681.22  \n",
      "98                   2.00            0.23               12413.17  \n",
      "26                   3.01            0.73               10881.18  \n",
      "101                  2.00            0.23               10569.57  \n",
      "57                   1.21            0.21               10358.05  \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Section D: SKU Analysis Table (Enhanced)\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Your original, correct aggregation\n",
    "sku_analysis = df.groupby(['product_id', 'description', 'purchasing_group', 'supplier_id']).agg(\n",
    "    quantity_purchased=('quantity', 'sum'),\n",
    "    net_value_spent=('net_value', 'sum'),\n",
    "    avg_price_paid=('unit_price', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# 2. Find the total quantity purchased for each SKU to calculate contribution\n",
    "sku_total_quantity = df.groupby('product_id')['quantity'].sum().reset_index()\n",
    "sku_total_quantity.rename(columns={'quantity': 'total_sku_quantity'}, inplace=True)\n",
    "\n",
    "# 3. Find the best (minimum) price available for each SKU across all suppliers\n",
    "best_prices = df.groupby('product_id')['unit_price'].min().reset_index()\n",
    "best_prices.rename(columns={'unit_price': 'best_available_price'}, inplace=True)\n",
    "\n",
    "# 4. Merge these new context columns into the main analysis table\n",
    "sku_analysis = pd.merge(sku_analysis, sku_total_quantity, on='product_id')\n",
    "sku_analysis = pd.merge(sku_analysis, best_prices, on='product_id')\n",
    "\n",
    "# 5. Calculate the new analysis columns\n",
    "sku_analysis['contribution_to_total_qty_%'] = (sku_analysis['quantity_purchased'] / sku_analysis['total_sku_quantity']) * 100\n",
    "sku_analysis['price_variance'] = sku_analysis['avg_price_paid'] - sku_analysis['best_available_price']\n",
    "sku_analysis['cost_above_best_price'] = sku_analysis['price_variance'] * sku_analysis['quantity_purchased']\n",
    "\n",
    "# 6. Clean up and sort the final table to show the largest cost variances first\n",
    "sku_analysis = sku_analysis[[\n",
    "    'purchasing_group', 'product_id', 'description', 'supplier_id',\n",
    "    'quantity_purchased', 'contribution_to_total_qty_%',\n",
    "    'avg_price_paid', 'best_available_price', 'price_variance', 'cost_above_best_price'\n",
    "]].sort_values('cost_above_best_price', ascending=False)\n",
    "\n",
    "print(\"\\nEnhanced SKU Analysis Table (showing highest overspending first):\")\n",
    "print(sku_analysis.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f867da53-c951-4aeb-bf6d-b4d95493c4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Section E: Supplier Risk Analysis ---\n",
      "\n",
      "Critical Single-Sourced SKUs (High Value, High Risk):\n",
      "Empty DataFrame\n",
      "Columns: [product_id, description, supplier_id, total_net_value]\n",
      "Index: []\n",
      "\n",
      "Top 5 Most Volatile SKUs by Price:\n",
      "  product_id          description  mean  std  CV_%\n",
      "6   MAT-6214           Wheat Bran  0.72 0.12 17.18\n",
      "0   MAT-3159             Molasses  0.84 0.12 14.50\n",
      "1   MAT-4077  Rumen Protected Fat  3.80 0.44 11.56\n",
      "4   MAT-5627              Alfalfa  0.97 0.09  9.83\n",
      "7   MAT-7729         Barley Grain  1.21 0.11  9.10\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Section E: Supplier Performance & Risk Management\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# 1. Supplier Concentration Risk (for High-Value Items)\n",
    "# We'll identify single-sourced SKUs that are in the top 80% of spend.\n",
    "top_spend_skus = sku_summary[sku_summary['cum_spend_pct'] <= 80]['product_id']\n",
    "single_sourced_mask = sku_summary['product_id'].isin(top_spend_skus) & (sku_summary['supplier_count'] == 1)\n",
    "critical_single_sourced_skus = sku_summary[single_sourced_mask]\n",
    "\n",
    "# Merge with supplier info to show who the single supplier is\n",
    "supplier_info = df[['product_id', 'supplier_id']].drop_duplicates()\n",
    "critical_risk_suppliers = pd.merge(critical_single_sourced_skus, supplier_info, on='product_id')\n",
    "\n",
    "# Clean up the final table\n",
    "critical_risk_suppliers = critical_risk_suppliers[[\n",
    "    'product_id', 'description', 'supplier_id', 'total_net_value'\n",
    "]].sort_values('total_net_value', ascending=False)\n",
    "\n",
    "\n",
    "# 2. Price Volatility Analysis\n",
    "# We use the Coefficient of Variation (CV = std dev / mean) to measure volatility.\n",
    "# A higher CV means more unstable prices.\n",
    "price_volatility = df.groupby(['product_id', 'description'])['unit_price'].agg(['mean', 'std']).reset_index()\n",
    "price_volatility['CV_%'] = (price_volatility['std'] / price_volatility['mean']) * 100\n",
    "price_volatility = price_volatility.sort_values('CV_%', ascending=False).fillna(0)\n",
    "\n",
    "\n",
    "# Export section for the new dataframes\n",
    "# with pd.ExcelWriter(\"procurement_dashboard_final.xlsx\", mode='a', engine='openpyxl') as writer:\n",
    "#     critical_risk_suppliers.to_excel(writer, sheet_name=\"E_Critical_Supplier_Risk\", index=False)\n",
    "#     price_volatility.to_excel(writer, sheet_name=\"E_Price_Volatility\", index=False)\n",
    "\n",
    "print(\"\\n--- Section E: Supplier Risk Analysis ---\")\n",
    "print(\"\\nCritical Single-Sourced SKUs (High Value, High Risk):\")\n",
    "print(critical_risk_suppliers.head())\n",
    "\n",
    "print(\"\\nTop 5 Most Volatile SKUs by Price:\")\n",
    "print(price_volatility.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac24353-d72b-4e4b-860d-c769b5aa15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Section F: Demand Forecasting ---\n",
      "\n",
      "3-Month Demand Forecast for Top 5 SKUs:\n",
      "  product_id        Forecast_Date  Forecast_Quantity\n",
      "0   MAT-6002  2026-01-31 00:00:00            8185.97\n",
      "1   MAT-6002  2026-02-28 00:00:00            8185.97\n",
      "2   MAT-6002  2026-03-31 00:00:00            8185.97\n",
      "0   MAT-5204                   10            5566.00\n",
      "1   MAT-5204                   11            5566.00\n",
      "2   MAT-5204                   12            5566.00\n",
      "0   MAT-4862                    7           16071.44\n",
      "1   MAT-4862                    8           16071.44\n",
      "2   MAT-4862                    9           16071.44\n",
      "0   MAT-6214  2026-01-31 00:00:00           13507.35\n",
      "1   MAT-6214  2026-02-28 00:00:00           13507.35\n",
      "2   MAT-6214  2026-03-31 00:00:00           13507.35\n",
      "0   MAT-3159  2026-01-31 00:00:00           10538.20\n",
      "1   MAT-3159  2026-02-28 00:00:00           10538.20\n",
      "2   MAT-3159  2026-03-31 00:00:00           10538.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazel\\AppData\\Local\\Temp\\ipykernel_18396\\2049901187.py:14: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_demand_ts = monthly_demand.groupby(['product_id', pd.Grouper(key='created_date', freq='M')])['quantity'].sum().reset_index()\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency ME will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency ME will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Nazel\\OneDrive\\Desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency ME will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    }
   ],
   "source": [
    "# You may need to install this library: pip install statsmodels\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Section F: Demand Forecasting\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# We will forecast demand for the Top 5 SKUs by quantity\n",
    "top_5_skus_by_qty = df.groupby('product_id')['quantity'].sum().nlargest(5).index.tolist()\n",
    "\n",
    "# 1. Prepare the monthly time-series data for these SKUs\n",
    "monthly_demand = df[df['product_id'].isin(top_5_skus_by_qty)].copy()\n",
    "monthly_demand['created_date'] = pd.to_datetime(monthly_demand['created_date'])\n",
    "monthly_demand_ts = monthly_demand.groupby(['product_id', pd.Grouper(key='created_date', freq='M')])['quantity'].sum().reset_index()\n",
    "\n",
    "# 2. Loop through each top SKU and generate a forecast\n",
    "all_forecasts = []\n",
    "for sku in top_5_skus_by_qty:\n",
    "    # Get the historical data for this one SKU\n",
    "    sku_history = monthly_demand_ts[monthly_demand_ts['product_id'] == sku].set_index('created_date')['quantity']\n",
    "    \n",
    "    # If there's enough data, create and fit the model\n",
    "    if len(sku_history) > 2:\n",
    "        # Fit the Simple Exponential Smoothing model\n",
    "        model = SimpleExpSmoothing(sku_history, initialization_method=\"estimated\").fit()\n",
    "        # Forecast the next 3 months\n",
    "        forecast = model.forecast(3).rename(f\"{sku}_forecast\").reset_index()\n",
    "        forecast.columns = ['Forecast_Date', 'Forecast_Quantity']\n",
    "        forecast['product_id'] = sku\n",
    "        all_forecasts.append(forecast)\n",
    "\n",
    "# 3. Combine all forecasts into a single DataFrame\n",
    "if all_forecasts:\n",
    "    demand_forecast_df = pd.concat(all_forecasts)\n",
    "    demand_forecast_df = demand_forecast_df[['product_id', 'Forecast_Date', 'Forecast_Quantity']]\n",
    "    \n",
    "    print(\"\\n--- Section F: Demand Forecasting ---\")\n",
    "    print(\"\\n3-Month Demand Forecast for Top 5 SKUs:\")\n",
    "    print(demand_forecast_df)\n",
    "\n",
    "    # Export section for the new dataframe\n",
    "    # with pd.ExcelWriter(\"procurement_dashboard_final.xlsx\", mode='a', engine='openpyxl') as writer:\n",
    "    #     demand_forecast_df.to_excel(writer, sheet_name=\"F_Demand_Forecast\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605fa55c-6c5c-4b21-a9bf-3444160ffb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.5-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from statsmodels) (2.3.3)\n",
      "Collecting scipy!=1.9.2,>=1.8 (from statsmodels)\n",
      "  Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from statsmodels) (2.3.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nazel\\onedrive\\desktop\\nadec-analysis\\procurement_sku_analysis\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Using cached statsmodels-0.14.5-cp313-cp313-win_amd64.whl (9.6 MB)\n",
      "Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "Installing collected packages: scipy, patsy, statsmodels\n",
      "\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ---------------------------------------- 0/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [patsy]\n",
      "   ------------- -------------------------- 1/3 [patsy]\n",
      "   ------------- -------------------------- 1/3 [patsy]\n",
      "   ------------- -------------------------- 1/3 [patsy]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   -------------------------- ------------- 2/3 [statsmodels]\n",
      "   ---------------------------------------- 3/3 [statsmodels]\n",
      "\n",
      "Successfully installed patsy-1.0.1 scipy-1.16.2 statsmodels-0.14.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6c3ce5-40c9-4385-b776-483f94cd3d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73840480-ab59-4767-9eeb-0add7a6bb256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
